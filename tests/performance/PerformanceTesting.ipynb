{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install aiohttp\n",
    "# %pip install mysql-connector-python\n",
    "# %pip install nest_asyncio\n",
    "# %pip install lenskit --upgrade\n",
    "# %pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n",
    "import urllib\n",
    "from pandas.io import sql\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import requests\n",
    "from time import perf_counter\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigReader:\n",
    "    def get_value(self, key):\n",
    "        with open('config.json') as json_data_file:\n",
    "            data = json.load(json_data_file)\n",
    "        return data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbManager:\n",
    "    def __init__(self):\n",
    "        reader = ConfigReader()\n",
    "        db_connection = reader.get_value(\"db_connection\")        \n",
    "        self.conn_string = '{db_engine}{connector}://{user}:{password}@{server}/{database}?port={port}'.format(\n",
    "            db_engine=db_connection['db_engine'],\n",
    "            connector=db_connection['connector'],\n",
    "            user=db_connection['user'],\n",
    "            password=db_connection['password'],\n",
    "            server=db_connection['server'],\n",
    "            database=db_connection['database'],\n",
    "            port=db_connection['port'])\n",
    "\n",
    "    def get_users(self):\n",
    "        # postgres:\n",
    "        return sql.read_sql(\"SELECT distinct \\\"user\\\" FROM rating;\", create_engine(self.conn_string))\n",
    "        # mysql:\n",
    "#        return sql.read_sql(\"SELECT distinct user FROM rating;\", create_engine(self.conn_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test prediction and recommendation endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and recommendations methods with semaphore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "throughputs = []\n",
    "\n",
    "def print_stats(times, time_taken_all, num_requests):\n",
    "    print(f'Total response time: {round(time_taken_all, 3)}')\n",
    "    print(f'Throughput (requests per second): {round(num_requests / time_taken_all, 3)}')\n",
    "    print(f'Peak response time: {round(max(times), 3)}')\n",
    "    print(f'Mean response time: {round(np.mean(times), 3)}')\n",
    "    print(f'99 percentile: {round(np.quantile(times, 0.99), 3)}')\n",
    "\n",
    "def plot_numbers(file_name):\n",
    "    resp_time_per_request = np.genfromtxt(file_name, delimiter=',')\n",
    "    plt.plot(resp_time_per_request)\n",
    "    plt.show()\n",
    "    \n",
    "def hist_numbers(file_name):\n",
    "    resp_time_per_request = np.genfromtxt(file_name, delimiter=',')\n",
    "    plt.hist(resp_time_per_request, bins='auto')\n",
    "    plt.show()\n",
    "\n",
    "# Predictions    \n",
    "async def get_preds_sem(num_sem, algo_pred, file_name=None, add_throughput=False):\n",
    "    times = []\n",
    "    sem = asyncio.Semaphore(num_sem)\n",
    "    tasks = []    \n",
    "    num_requests = len(n_rand_users)\n",
    "    print(f'Number of requests: {num_requests}')\n",
    "    start_preds = perf_counter()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in n_rand_users.iterrows():\n",
    "            task = asyncio.ensure_future(get_user_preds_with_sem(row['user'], algo_pred, items, session, sem, times))\n",
    "            tasks.append(task)         \n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        time_taken_all = perf_counter() - start_preds\n",
    "        print_stats(times, time_taken_all, num_requests)\n",
    "        \n",
    "        if file_name != None and file_name != '':\n",
    "            if os.path.exists(file_name):\n",
    "                os.remove(file_name)\n",
    "            np.savetxt(file_name, times, delimiter=',')\n",
    "        \n",
    "        if add_throughput:\n",
    "            throughputs.append(num_requests / time_taken_all)\n",
    "\n",
    "async def get_user_preds_with_sem(user, algo, items, session, sem, times):\n",
    "    async with sem:  # semaphore limits num of simultaneous downloads\n",
    "        return await get_user_preds_sem(user, algo, items, session, times)        \n",
    "        \n",
    "async def get_user_preds_sem(user, algo, items, session, times):\n",
    "    url = f'{base_url}/algorithms/{algo}/predictions?user_id={user}&items={items}'\n",
    "    start = perf_counter()\n",
    "    async with session.get(url) as resp:\n",
    "        data = await resp.json()    \n",
    "        time_taken = perf_counter() - start\n",
    "        times.append(time_taken)\n",
    "        \n",
    "# Recommendations\n",
    "async def get_recs_sem(num_sem, algo_rec, file_name=None, add_throughput=False):\n",
    "    times = []\n",
    "    sem = asyncio.Semaphore(num_sem)\n",
    "    tasks = []\n",
    "    num_requests = len(n_rand_users)\n",
    "    print(f'Number of requests: {num_requests}')\n",
    "    start_preds = perf_counter()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in n_rand_users.iterrows():\n",
    "            task = asyncio.ensure_future(get_user_recs_with_sem(row['user'], algo_rec, n_recs, session, sem, times))\n",
    "            tasks.append(task)         \n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        time_taken_all = perf_counter() - start_preds\n",
    "        print_stats(times, time_taken_all, num_requests)\n",
    "        \n",
    "        if file_name != None and file_name != '':\n",
    "            if os.path.exists(file_name):\n",
    "                os.remove(file_name)\n",
    "            np.savetxt(file_name, times, delimiter=',')\n",
    "        \n",
    "        if add_throughput:\n",
    "            throughputs.append(num_requests / time_taken_all)\n",
    "\n",
    "async def get_user_recs_with_sem(user, algo, n_recs, session, sem, times):\n",
    "    async with sem:  # semaphore limits num of simultaneous downloads\n",
    "        return await get_user_preds_sem(user, algo, n_recs, session, times)        \n",
    "        \n",
    "async def get_user_recs_sem(user, algo, n_recs, session, times):\n",
    "    url = f'{base_url}/algorithms/{algo}/recommendations?user_id={user}&num_recs={n_recs}'\n",
    "    start = perf_counter()\n",
    "    async with session.get(url) as resp:\n",
    "        data = await resp.json()    \n",
    "        time_taken = perf_counter() - start\n",
    "        times.append(time_taken)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gunicorn methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def get_gunicorn_master_pid():\n",
    "    proc1 = subprocess.Popen(['ps', 'ax'], stdout=subprocess.PIPE)\n",
    "    proc2 = subprocess.Popen(['grep', 'gunicorn'], stdin=proc1.stdout,\n",
    "                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    proc1.stdout.close() # Allow proc1 to receive a SIGPIPE if proc2 exits.\n",
    "    out, err = proc2.communicate()\n",
    "    process_length = ConfigReader().get_value('process_length')\n",
    "    master_id = out[:process_length].decode('utf-8').replace(' ', '')\n",
    "    return master_id\n",
    "\n",
    "def add_workers(n):\n",
    "    master_id = get_gunicorn_master_pid()\n",
    "    for i in range(n):\n",
    "        os.system(f\"sudo kill -s TTIN {master_id}\")\n",
    "        \n",
    "def remove_workers(n):\n",
    "    master_id = get_gunicorn_master_pid()\n",
    "    for i in range(n):\n",
    "        os.system(f\"sudo kill -s TTOU {master_id}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ConfigReader()\n",
    "n_rand_users = num_requests = reader.get_value(\"num_requests\")\n",
    "dbManager = DbManager()\n",
    "db_users = dbManager.get_users()\n",
    "n_rand_users = db_users.sample(n=n_rand_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = reader.get_value(\"rec_server_baese_url\")\n",
    "n_recs = reader.get_value(\"n_recs\")\n",
    "items = reader.get_value(\"items\")\n",
    "pred_algos = reader.get_value(\"pred_algos\")\n",
    "rec_algos = reader.get_value(\"rec_algos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def warm_up_async(current_algo=None, num_workers=24, display_logs=True):\n",
    "    warm_up_user = 1\n",
    "    times = []\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for algo in pred_algos:\n",
    "            if current_algo is None or algo == current_algo:\n",
    "                for w in range(num_workers * 2):\n",
    "                    if display_logs:\n",
    "                        print(f'Calling {algo}. Worker number: {w + 1}')\n",
    "                    task = asyncio.ensure_future(get_user_preds_sem(warm_up_user, algo, items, session, times))\n",
    "                    tasks.append(task)\n",
    "        responses = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warm_up(current_algo=None, num_workers=24, display_logs=True):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(warm_up_async(current_algo, num_workers, display_logs))\n",
    "    loop.run_until_complete(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling bias. Worker number: 1\n",
      "Calling bias. Worker number: 2\n",
      "Calling bias. Worker number: 3\n",
      "Calling bias. Worker number: 4\n",
      "Calling bias. Worker number: 5\n",
      "Calling bias. Worker number: 6\n",
      "Calling bias. Worker number: 7\n",
      "Calling bias. Worker number: 8\n",
      "Calling itemitem. Worker number: 1\n",
      "Calling itemitem. Worker number: 2\n",
      "Calling itemitem. Worker number: 3\n",
      "Calling itemitem. Worker number: 4\n",
      "Calling itemitem. Worker number: 5\n",
      "Calling itemitem. Worker number: 6\n",
      "Calling itemitem. Worker number: 7\n",
      "Calling itemitem. Worker number: 8\n",
      "Calling useruser. Worker number: 1\n",
      "Calling useruser. Worker number: 2\n",
      "Calling useruser. Worker number: 3\n",
      "Calling useruser. Worker number: 4\n",
      "Calling useruser. Worker number: 5\n",
      "Calling useruser. Worker number: 6\n",
      "Calling useruser. Worker number: 7\n",
      "Calling useruser. Worker number: 8\n",
      "Calling biasedmf. Worker number: 1\n",
      "Calling biasedmf. Worker number: 2\n",
      "Calling biasedmf. Worker number: 3\n",
      "Calling biasedmf. Worker number: 4\n",
      "Calling biasedmf. Worker number: 5\n",
      "Calling biasedmf. Worker number: 6\n",
      "Calling biasedmf. Worker number: 7\n",
      "Calling biasedmf. Worker number: 8\n",
      "Calling implicitmf. Worker number: 1\n",
      "Calling implicitmf. Worker number: 2\n",
      "Calling implicitmf. Worker number: 3\n",
      "Calling implicitmf. Worker number: 4\n",
      "Calling implicitmf. Worker number: 5\n",
      "Calling implicitmf. Worker number: 6\n",
      "Calling implicitmf. Worker number: 7\n",
      "Calling implicitmf. Worker number: 8\n",
      "Calling funksvd. Worker number: 1\n",
      "Calling funksvd. Worker number: 2\n",
      "Calling funksvd. Worker number: 3\n",
      "Calling funksvd. Worker number: 4\n",
      "Calling funksvd. Worker number: 5\n",
      "Calling funksvd. Worker number: 6\n",
      "Calling funksvd. Worker number: 7\n",
      "Calling funksvd. Worker number: 8\n",
      "Calling tf_bpr. Worker number: 1\n",
      "Calling tf_bpr. Worker number: 2\n",
      "Calling tf_bpr. Worker number: 3\n",
      "Calling tf_bpr. Worker number: 4\n",
      "Calling tf_bpr. Worker number: 5\n",
      "Calling tf_bpr. Worker number: 6\n",
      "Calling tf_bpr. Worker number: 7\n",
      "Calling tf_bpr. Worker number: 8\n"
     ]
    }
   ],
   "source": [
    "warm_up(None, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call predict and recommend endpoints from server for canonical config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions for different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: bias\n",
      "Number of requests: 2\n",
      "Total response time: 0.133\n",
      "Throughput (requests per second): 15.018\n",
      "Peak response time: 0.131\n",
      "Mean response time: 0.129\n",
      "99 percentile: 0.131\n",
      "---------------------\n",
      "\n",
      "Algorithm: itemitem\n",
      "Number of requests: 2\n",
      "Total response time: 0.303\n",
      "Throughput (requests per second): 6.607\n",
      "Peak response time: 0.302\n",
      "Mean response time: 0.231\n",
      "99 percentile: 0.3\n",
      "---------------------\n",
      "\n",
      "Algorithm: useruser\n",
      "Number of requests: 2\n",
      "Total response time: 0.124\n",
      "Throughput (requests per second): 16.072\n",
      "Peak response time: 0.124\n",
      "Mean response time: 0.12\n",
      "99 percentile: 0.124\n",
      "---------------------\n",
      "\n",
      "Algorithm: biasedmf\n",
      "Number of requests: 2\n",
      "Total response time: 0.128\n",
      "Throughput (requests per second): 15.626\n",
      "Peak response time: 0.127\n",
      "Mean response time: 0.121\n",
      "99 percentile: 0.127\n",
      "---------------------\n",
      "\n",
      "Algorithm: implicitmf\n",
      "Number of requests: 2\n",
      "Total response time: 0.154\n",
      "Throughput (requests per second): 12.988\n",
      "Peak response time: 0.153\n",
      "Mean response time: 0.152\n",
      "99 percentile: 0.153\n",
      "---------------------\n",
      "\n",
      "Algorithm: funksvd\n",
      "Number of requests: 2\n",
      "Total response time: 0.175\n",
      "Throughput (requests per second): 11.439\n",
      "Peak response time: 0.174\n",
      "Mean response time: 0.174\n",
      "99 percentile: 0.174\n",
      "---------------------\n",
      "\n",
      "Algorithm: tf_bpr\n",
      "Number of requests: 2\n",
      "Total response time: 0.232\n",
      "Throughput (requests per second): 8.623\n",
      "Peak response time: 0.231\n",
      "Mean response time: 0.231\n",
      "99 percentile: 0.231\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for algo in pred_algos:\n",
    "    file_name = f'preds_{algo}_workers_4_num_req_{num_requests}.csv'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    print(f'Algorithm: {algo}')\n",
    "    future = asyncio.ensure_future(get_preds_sem(8, algo, file_name, True))\n",
    "    loop.run_until_complete(future)\n",
    "#    plot_numbers(file_name)\n",
    "#    hist_numbers(file_name)\n",
    "    print('---------------------')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: popular\n",
      "Number of requests: 2\n",
      "Total response time: 0.023\n",
      "Throughput (requests per second): 86.446\n",
      "Peak response time: 0.022\n",
      "Mean response time: 0.022\n",
      "99 percentile: 0.022\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "algo_rec = 'popular'\n",
    "print(f'Algorithm: {algo_rec}')\n",
    "file_name = f'recs_{algo_rec}_workers_4_num_req_{num_requests}.csv'\n",
    "loop = asyncio.get_event_loop()\n",
    "future = asyncio.ensure_future(get_recs_sem(8, algo_rec, file_name))\n",
    "loop.run_until_complete(future)\n",
    "print('---------------------')\n",
    "print('')\n",
    "#plot_numbers(file_name)\n",
    "#hist_numbers(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lenskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "from binpickle import BinPickleFile\n",
    "from pathlib import Path\n",
    "\n",
    "directory_path = 'models'\n",
    "\n",
    "def exists_model_file(algo):\n",
    "    full_file_name = Path(directory_path) / algo\n",
    "    if full_file_name.exists():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def load_for_shared_mem(file_name):\n",
    "    full_file_name = Path(directory_path) / file_name\n",
    "\n",
    "    binpickle_file = BinPickleFile(full_file_name, direct=True)\n",
    "    model = binpickle_file.load()\n",
    "    return model\n",
    "\n",
    "def get_predictions_from_model(model, user, items):\n",
    "    try:\n",
    "        results = []\n",
    "        df_preds = model.predict_for_user(user, items)\n",
    "        for index, value in df_preds.iteritems():\n",
    "            if not math.isnan(value):\n",
    "                results.append({'item': index, 'score': value})\n",
    "        return results\n",
    "    except:\n",
    "        print(f\"Unexpected preds error for user: {user}, with items: {items}. Error: {sys.exc_info()[0]}\")\n",
    "        raise\n",
    "        \n",
    "\n",
    "# Predictions    \n",
    "async def get_preds_threads_lkpy(num_sem, model, file_name=None, add_throughput=False):\n",
    "    times = []\n",
    "    sem = asyncio.Semaphore(num_sem)\n",
    "    tasks = []    \n",
    "    num_requests = len(n_rand_users)\n",
    "    print(f'Number of requests: {num_requests}')\n",
    "    start_preds = perf_counter()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in n_rand_users.iterrows():\n",
    "            task = asyncio.ensure_future(get_user_preds_with_threads_lkpy(row['user'], items, session, sem, times, model))\n",
    "            tasks.append(task)         \n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        time_taken_all = perf_counter() - start_preds\n",
    "        print_stats(times, time_taken_all, num_requests)\n",
    "        \n",
    "        if file_name != None and file_name != '':\n",
    "            if os.path.exists(file_name):\n",
    "                os.remove(file_name)\n",
    "            np.asarray(times)\n",
    "            np.savetxt(file_name, times, delimiter=',')\n",
    "        \n",
    "        if add_throughput:\n",
    "            throughputs.append(num_requests / time_taken_all)\n",
    "\n",
    "async def get_user_preds_with_threads_lkpy(user, items, session, sem, times, model):\n",
    "    async with sem:  # semaphore limits num of simultaneous downloads\n",
    "        return await get_user_preds_threads_lkpy(user, items, session, times, model)        \n",
    "        \n",
    "async def get_user_preds_threads_lkpy(user, items, session, times, model):\n",
    "    try:\n",
    "        start = perf_counter()\n",
    "        results = []\n",
    "        df_preds = model.predict_for_user(user, list(map(int, items.split(','))))\n",
    "        for index, value in df_preds.iteritems():\n",
    "            if not math.isnan(value):\n",
    "                results.append({'item': index, 'score': value})\n",
    "                \n",
    "        time_taken = perf_counter() - start\n",
    "        times.append(time_taken)\n",
    "        return results\n",
    "    except:\n",
    "        print(f\"Unexpected preds error for user: {user}, with items: {items}. Error: {sys.exc_info()[0]}\")\n",
    "        raise        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/anaconda3/lib/python3.7/site-packages/fastparquet/encoding.py:222: NumbaDeprecationWarning: \u001b[1mThe 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\u001b[0m\n",
      "  Numpy8 = numba.jitclass(spec8)(NumpyIO)\n",
      "/Users/carlos/anaconda3/lib/python3.7/site-packages/fastparquet/encoding.py:224: NumbaDeprecationWarning: \u001b[1mThe 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\u001b[0m\n",
      "  Numpy32 = numba.jitclass(spec32)(NumpyIO)\n",
      "/Users/carlos/anaconda3/lib/python3.7/site-packages/fastparquet/dataframe.py:5: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import CategoricalIndex, RangeIndex, Index, MultiIndex\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from file\n",
      "Creating model for bias\n",
      "Model bias saved successfully\n",
      "Creating model for biasedmf\n",
      "Model biasedmf saved successfully\n",
      "Creating model for itemitem\n",
      "Model itemitem saved successfully\n",
      "Creating model for tf_bpr\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "TensorFlow not usable, too old?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2d52d378c838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlk_recserver_algos_not_created\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk_recserver_algos_not_created\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_save_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk_recserver_algos_not_created\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/BSU/Research/Project/web_apps/rec-service/tests/performance/train_save_model.py\u001b[0m in \u001b[0;36msave_models\u001b[0;34m(algos)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Creating model for {algo}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".bpk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BSU/Research/Project/web_apps/rec-service/tests/performance/train_save_model.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(algo, ratings)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0malgo_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topn_algo_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0malgo_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_algo_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0malgo_class\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BSU/Research/Project/web_apps/rec-service/tests/performance/train_save_model.py\u001b[0m in \u001b[0;36mget_algo_class\u001b[0;34m(algo)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunkSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0malgo\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf_bpr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlktf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBPR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topn_algo_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BSU/Research/Lenskit/lkpy/lenskit/algorithms/tf/bpr.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, features, epochs, batch_size, reg, neg_count, rng_spec)\u001b[0m\n\u001b[1;32m    138\u001b[0m     def __init__(self, features=50, *, epochs=5, batch_size=10000,\n\u001b[1;32m    139\u001b[0m                  reg=0.02, neg_count=1, rng_spec=None):\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mcheck_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BSU/Research/Lenskit/lkpy/lenskit/algorithms/tf/util.py\u001b[0m in \u001b[0;36mcheck_tensorflow\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhave_usable_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TensorFlow not usable, too old?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: TensorFlow not usable, too old?"
     ]
    }
   ],
   "source": [
    "import train_save_model\n",
    "lk_recserver_algos = reader.get_value('lk_recserver_algos')\n",
    "lk_recserver_algos_not_created = []\n",
    "for a in lk_recserver_algos:\n",
    "    if not exists_model_file(f'{a}.bpk'):\n",
    "        lk_recserver_algos_not_created.append(a)\n",
    "if len(lk_recserver_algos_not_created) > 0:\n",
    "    train_save_model.save_models(lk_recserver_algos_not_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lenskit performance:')\n",
    "for lk_recserver_algo in lk_recserver_algos:\n",
    "    print(f'Algo: {lk_recserver_algo}')\n",
    "    model = load_for_shared_mem(f'{lk_recserver_algo}.bpk')\n",
    "    file_name = f'lkpy_{lk_recserver_algo}_num_req_{num_requests}.csv'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_preds_threads_lkpy(8, model, file_name))\n",
    "    loop.run_until_complete(future)\n",
    "    #plot_numbers(file_name)\n",
    "    #hist_numbers(file_name)\n",
    "    print('------------------')    \n",
    "    warm_up(lk_recserver_algo, 8, False)\n",
    "    print('Recommendation server performance:')\n",
    "    file_name = f'preds_{lk_recserver_algo}_against_lkpy_workers_4_num_req_{num_requests}.csv'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_preds_sem(8, lk_recserver_algo, file_name, True))\n",
    "    loop.run_until_complete(future)\n",
    "    #plot_numbers(file_name)\n",
    "    #hist_numbers(file_name)\n",
    "    print('*******************************************************')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speedup Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughputs = []\n",
    "linear_speedup_algos = reader.get_value(\"linear_speedup_algos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_server(file_name):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_preds_sem(8, current_algo, file_name, True))\n",
    "    loop.run_until_complete(future)\n",
    "#    plot_numbers(file_name)\n",
    "#    hist_numbers(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_config = reader.get_value(\"workers_config\")\n",
    "inc_config = reader.get_value(\"inc_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_algo in linear_speedup_algos:\n",
    "    i = 0\n",
    "    throughputs = []\n",
    "    remove_workers(3) # reduce from 4 workers to 1\n",
    "    for num_workers in workers_config:\n",
    "        print(f'Algo: {current_algo}, Workers: {num_workers}')\n",
    "        warm_up(current_algo, num_workers, display_logs=False)\n",
    "        file_name = f'linear_speedup_preds_{current_algo}_workers_{num_workers}_num_req_{num_requests}.csv'\n",
    "        call_server(file_name)\n",
    "        if (num_workers != workers_config[-1]):\n",
    "            print(f'add {inc_config[i]} workers')\n",
    "            add_workers(inc_config[i])\n",
    "        i += 1\n",
    "        print('------------------')\n",
    "    throughput_file_name_workers = f'throughput_single_multiple_workers_algo_{current_algo}.csv'\n",
    "    np.savetxt(throughput_file_name_workers, throughputs , delimiter=',')\n",
    "    remove_workers(workers_config[-1] - 4) # remove workers to get only 4 (default config)\n",
    "    print('*******************************************************')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Throughput by number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throughput_file_name_workers = 'throughput_single_multiple_workers.csv'\n",
    "# np.savetxt(throughput_file_name_workers, throughputs , delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throughputs_workers_from_file = np.genfromtxt(throughput_file_name_workers, delimiter=',')\n",
    "# workers = [1, 2, 4, 8, 16]\n",
    "# y_pos = np.arange(len(throughputs_workers_from_file))\n",
    "\n",
    "# plt.bar(y_pos, throughputs_workers_from_file, align='center', alpha=0.5)\n",
    "# plt.xticks(y_pos, workers)\n",
    "# plt.ylabel('Throughput')\n",
    "# plt.title('Throughput by workers')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and recommend endpoints from server for canonical config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions for different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for algo in pred_algos:\n",
    "#     file_name = f'preds_{algo}_parallel_threads_8_workers_4_num_req_{num_requests}.csv'  \n",
    "#     plot_numbers(file_name)\n",
    "#     hist_numbers(file_name)\n",
    "#     print('---------------------')\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
