{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install aiohttp\n",
    "# %pip install mysql-connector-python\n",
    "# %pip install nest_asyncio\n",
    "# %pip install lenskit --upgrade\n",
    "# %pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n",
    "import urllib\n",
    "from pandas.io import sql\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import requests\n",
    "from time import perf_counter\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigReader:\n",
    "    def get_value(self, key):\n",
    "        with open('config.json') as json_data_file:\n",
    "            data = json.load(json_data_file)\n",
    "        return data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbManager:\n",
    "    def __init__(self):\n",
    "        reader = ConfigReader()\n",
    "        db_connection = reader.get_value(\"db_connection\")        \n",
    "        self.conn_string = '{db_engine}{connector}://{user}:{password}@{server}/{database}?port={port}'.format(\n",
    "            db_engine=db_connection['db_engine'],\n",
    "            connector=db_connection['connector'],\n",
    "            user=db_connection['user'],\n",
    "            password=db_connection['password'],\n",
    "            server=db_connection['server'],\n",
    "            database=db_connection['database'],\n",
    "            port=db_connection['port'])\n",
    "\n",
    "    def get_users(self):\n",
    "        # postgres:\n",
    "#        return sql.read_sql(\"SELECT distinct \\\"user\\\" FROM rating;\", create_engine(self.conn_string))\n",
    "        # mysql:\n",
    "        return sql.read_sql(\"SELECT distinct user FROM rating;\", create_engine(self.conn_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test recommendation endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semaphore performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "throughputs = []\n",
    "\n",
    "def print_stats(times, time_taken_all, num_requests):\n",
    "    print(f'Total response time: {round(time_taken_all, 3)}')\n",
    "    print(f'Throughput (requests per second): {round(num_requests / time_taken_all, 3)}')\n",
    "    print(f'Peak response time: {round(max(times), 3)}')\n",
    "    print(f'Mean response time: {round(np.mean(times), 3)}')\n",
    "    print(f'99 percentile: {round(np.quantile(times, 0.99), 3)}')\n",
    "\n",
    "def plot_numbers(file_name):\n",
    "    resp_time_per_request = np.genfromtxt(file_name, delimiter=',')\n",
    "    plt.plot(resp_time_per_request)\n",
    "    plt.show()\n",
    "    \n",
    "def hist_numbers(file_name):\n",
    "    resp_time_per_request = np.genfromtxt(file_name, delimiter=',')\n",
    "    plt.hist(resp_time_per_request, bins='auto')\n",
    "    plt.show()\n",
    "\n",
    "# Predictions    \n",
    "async def get_preds_sem(num_sem, algo_pred, file_name=None, add_throughput=False):\n",
    "    times = []\n",
    "    sem = asyncio.Semaphore(num_sem)\n",
    "    tasks = []    \n",
    "    num_requests = len(n_rand_users)\n",
    "    print(f'Number of requests: {num_requests}')\n",
    "    start_preds = perf_counter()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in n_rand_users.iterrows():\n",
    "            task = asyncio.ensure_future(get_user_preds_with_sem(row['user'], algo_pred, items, session, sem, times))\n",
    "            tasks.append(task)         \n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        time_taken_all = perf_counter() - start_preds\n",
    "        print_stats(times, time_taken_all, num_requests)\n",
    "        \n",
    "        if file_name != None and file_name != '':\n",
    "            if os.path.exists(file_name):\n",
    "                os.remove(file_name)\n",
    "            np.savetxt(file_name, times, delimiter=',')\n",
    "        \n",
    "        if add_throughput:\n",
    "            throughputs.append(num_requests / time_taken_all)\n",
    "\n",
    "async def get_user_preds_with_sem(user, algo, items, session, sem, times):\n",
    "    async with sem:  # semaphore limits num of simultaneous downloads\n",
    "        return await get_user_preds_sem(user, algo, items, session, times)        \n",
    "        \n",
    "async def get_user_preds_sem(user, algo, items, session, times):\n",
    "    url = f'{base_url}/algorithms/{algo}/predictions?user_id={user}&items={items}'\n",
    "    start = perf_counter()\n",
    "    async with session.get(url) as resp:\n",
    "        data = await resp.json()    \n",
    "        time_taken = perf_counter() - start\n",
    "        times.append(time_taken)\n",
    "        \n",
    "# Recommendations\n",
    "async def get_recs_sem(num_sem, algo_rec, file_name=None, add_throughput=False):\n",
    "    times = []\n",
    "    sem = asyncio.Semaphore(num_sem)\n",
    "    tasks = []\n",
    "    num_requests = len(n_rand_users)\n",
    "    print(f'Number of requests: {num_requests}')\n",
    "    start_preds = perf_counter()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in n_rand_users.iterrows():\n",
    "            task = asyncio.ensure_future(get_user_recs_with_sem(row['user'], algo_rec, n_recs, session, sem, times))\n",
    "            tasks.append(task)         \n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        time_taken_all = perf_counter() - start_preds\n",
    "        print_stats(times, time_taken_all, num_requests)\n",
    "        \n",
    "        if file_name != None and file_name != '':\n",
    "            if os.path.exists(file_name):\n",
    "                os.remove(file_name)\n",
    "            np.savetxt(file_name, times, delimiter=',')\n",
    "        \n",
    "        if add_throughput:\n",
    "            throughputs.append(num_requests / time_taken_all)\n",
    "\n",
    "async def get_user_recs_with_sem(user, algo, n_recs, session, sem, times):\n",
    "    async with sem:  # semaphore limits num of simultaneous downloads\n",
    "        return await get_user_preds_sem(user, algo, n_recs, session, times)        \n",
    "        \n",
    "async def get_user_recs_sem(user, algo, n_recs, session, times):\n",
    "    url = f'{base_url}/algorithms/{algo}/recommendations?user_id={user}&num_recs={n_recs}'\n",
    "    start = perf_counter()\n",
    "    async with session.get(url) as resp:\n",
    "        data = await resp.json()    \n",
    "        time_taken = perf_counter() - start\n",
    "        times.append(time_taken)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gunicorn methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def get_gunicorn_master_pid():\n",
    "    proc1 = subprocess.Popen(['ps', 'ax'], stdout=subprocess.PIPE)\n",
    "    proc2 = subprocess.Popen(['grep', 'gunicorn'], stdin=proc1.stdout,\n",
    "                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    proc1.stdout.close() # Allow proc1 to receive a SIGPIPE if proc2 exits.\n",
    "    out, err = proc2.communicate()\n",
    "    process_length = ConfigReader().get_value('process_length')\n",
    "    master_id = out[:process_length].decode('utf-8').replace(' ', '')\n",
    "    return master_id\n",
    "\n",
    "def add_workers(n):\n",
    "    master_id = get_gunicorn_master_pid()\n",
    "    for i in range(n):\n",
    "        os.system(f\"sudo kill -s TTIN {master_id}\")\n",
    "        \n",
    "def remove_workers(n):\n",
    "    master_id = get_gunicorn_master_pid()\n",
    "    for i in range(n):\n",
    "        os.system(f\"sudo kill -s TTOU {master_id}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ConfigReader()\n",
    "n_rand_users = num_requests = reader.get_value(\"num_requests\")\n",
    "dbManager = DbManager()\n",
    "db_users = dbManager.get_users()\n",
    "n_rand_users = db_users.sample(n=n_rand_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = reader.get_value(\"rec_server_baese_url\")\n",
    "n_recs = reader.get_value(\"n_recs\")\n",
    "items = reader.get_value(\"items\")\n",
    "pred_algos = reader.get_value(\"pred_algos\")\n",
    "rec_algos = reader.get_value(\"rec_algos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def warm_up_async(current_algo=None, num_workers=24, display_logs=True):\n",
    "    warm_up_user = 1\n",
    "    times = []\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for algo in pred_algos:\n",
    "            if current_algo is None or algo == current_algo:\n",
    "                for w in range(num_workers * 2):\n",
    "                    if display_logs:\n",
    "                        print(f'Calling {algo}. Worker number: {w + 1}')\n",
    "                    task = asyncio.ensure_future(get_user_preds_sem(warm_up_user, algo, items, session, times))\n",
    "                    tasks.append(task)\n",
    "        responses = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warm_up(current_algo=None, num_workers=24, display_logs=True):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(warm_up_async(current_algo, num_workers, display_logs))\n",
    "    loop.run_until_complete(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling bias. Worker number: 1\n",
      "Calling bias. Worker number: 2\n",
      "Calling bias. Worker number: 3\n",
      "Calling bias. Worker number: 4\n",
      "Calling bias. Worker number: 5\n",
      "Calling bias. Worker number: 6\n",
      "Calling bias. Worker number: 7\n",
      "Calling bias. Worker number: 8\n",
      "Calling itemitem. Worker number: 1\n",
      "Calling itemitem. Worker number: 2\n",
      "Calling itemitem. Worker number: 3\n",
      "Calling itemitem. Worker number: 4\n",
      "Calling itemitem. Worker number: 5\n",
      "Calling itemitem. Worker number: 6\n",
      "Calling itemitem. Worker number: 7\n",
      "Calling itemitem. Worker number: 8\n",
      "Calling useruser. Worker number: 1\n",
      "Calling useruser. Worker number: 2\n",
      "Calling useruser. Worker number: 3\n",
      "Calling useruser. Worker number: 4\n",
      "Calling useruser. Worker number: 5\n",
      "Calling useruser. Worker number: 6\n",
      "Calling useruser. Worker number: 7\n",
      "Calling useruser. Worker number: 8\n",
      "Calling biasedmf. Worker number: 1\n",
      "Calling biasedmf. Worker number: 2\n",
      "Calling biasedmf. Worker number: 3\n",
      "Calling biasedmf. Worker number: 4\n",
      "Calling biasedmf. Worker number: 5\n",
      "Calling biasedmf. Worker number: 6\n",
      "Calling biasedmf. Worker number: 7\n",
      "Calling biasedmf. Worker number: 8\n",
      "Calling implicitmf. Worker number: 1\n",
      "Calling implicitmf. Worker number: 2\n",
      "Calling implicitmf. Worker number: 3\n",
      "Calling implicitmf. Worker number: 4\n",
      "Calling implicitmf. Worker number: 5\n",
      "Calling implicitmf. Worker number: 6\n",
      "Calling implicitmf. Worker number: 7\n",
      "Calling implicitmf. Worker number: 8\n",
      "Calling funksvd. Worker number: 1\n",
      "Calling funksvd. Worker number: 2\n",
      "Calling funksvd. Worker number: 3\n",
      "Calling funksvd. Worker number: 4\n",
      "Calling funksvd. Worker number: 5\n",
      "Calling funksvd. Worker number: 6\n",
      "Calling funksvd. Worker number: 7\n",
      "Calling funksvd. Worker number: 8\n"
     ]
    }
   ],
   "source": [
    "warm_up(None, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call predict and recommend from server for canonical config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions for different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: bias\n",
      "Number of requests: 100\n",
      "Total response time: 4.374\n",
      "Throughput (requests per second): 22.865\n",
      "Peak response time: 0.462\n",
      "Mean response time: 0.338\n",
      "99 percentile: 0.446\n",
      "---------------------\n",
      "\n",
      "Algorithm: itemitem\n",
      "Number of requests: 100\n",
      "Total response time: 5.083\n",
      "Throughput (requests per second): 19.674\n",
      "Peak response time: 0.581\n",
      "Mean response time: 0.392\n",
      "99 percentile: 0.571\n",
      "---------------------\n",
      "\n",
      "Algorithm: useruser\n",
      "Number of requests: 100\n",
      "Total response time: 4.379\n",
      "Throughput (requests per second): 22.837\n",
      "Peak response time: 0.535\n",
      "Mean response time: 0.339\n",
      "99 percentile: 0.533\n",
      "---------------------\n",
      "\n",
      "Algorithm: biasedmf\n",
      "Number of requests: 100\n",
      "Total response time: 5.325\n",
      "Throughput (requests per second): 18.78\n",
      "Peak response time: 0.467\n",
      "Mean response time: 0.412\n",
      "99 percentile: 0.458\n",
      "---------------------\n",
      "\n",
      "Algorithm: implicitmf\n",
      "Number of requests: 100\n",
      "Total response time: 4.228\n",
      "Throughput (requests per second): 23.652\n",
      "Peak response time: 0.42\n",
      "Mean response time: 0.326\n",
      "99 percentile: 0.406\n",
      "---------------------\n",
      "\n",
      "Algorithm: funksvd\n",
      "Number of requests: 100\n",
      "Total response time: 3.956\n",
      "Throughput (requests per second): 25.279\n",
      "Peak response time: 0.337\n",
      "Mean response time: 0.307\n",
      "99 percentile: 0.336\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for algo in pred_algos:\n",
    "    file_name = f'preds_{algo}_parallel_threads_8_workers_4_num_req_{num_requests}.csv'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    print(f'Algorithm: {algo}')\n",
    "    future = asyncio.ensure_future(get_preds_sem(8, algo, file_name, True))\n",
    "    loop.run_until_complete(future)\n",
    "#    plot_numbers(file_name)\n",
    "#    hist_numbers(file_name)\n",
    "    print('---------------------')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: popular\n",
      "Number of requests: 100\n",
      "Total response time: 0.205\n",
      "Throughput (requests per second): 486.944\n",
      "Peak response time: 0.031\n",
      "Mean response time: 0.014\n",
      "99 percentile: 0.031\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "algo_rec = 'popular'\n",
    "print(f'Algorithm: {algo_rec}')\n",
    "file_name = f'recs_{algo_rec}_parallel_threads_8_workers_4_num_req_{num_requests}.csv'\n",
    "loop = asyncio.get_event_loop()\n",
    "future = asyncio.ensure_future(get_recs_sem(8, algo_rec, file_name))\n",
    "loop.run_until_complete(future)\n",
    "print('---------------------')\n",
    "print('')\n",
    "#plot_numbers(file_name)\n",
    "#hist_numbers(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speedup Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughputs = []\n",
    "linear_speedup_algos = reader.get_value(\"linear_speedup_algos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_server(file_name):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_preds_sem(8, current_algo, file_name, True))\n",
    "    loop.run_until_complete(future)\n",
    "#    plot_numbers(file_name)\n",
    "#    hist_numbers(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_config = reader.get_value(\"workers_config\")\n",
    "inc_config = reader.get_value(\"inc_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo: biasedmf, Workers: 1\n",
      "Number of requests: 100\n",
      "Total response time: 5.081\n",
      "Throughput (requests per second): 19.681\n",
      "Peak response time: 0.541\n",
      "Mean response time: 0.39\n",
      "99 percentile: 0.521\n",
      "add 1 workers\n",
      "------------------\n",
      "Algo: biasedmf, Workers: 2\n",
      "Number of requests: 100\n",
      "Total response time: 5.407\n",
      "Throughput (requests per second): 18.495\n",
      "Peak response time: 0.692\n",
      "Mean response time: 0.418\n",
      "99 percentile: 0.669\n",
      "add 2 workers\n",
      "------------------\n",
      "Algo: biasedmf, Workers: 4\n",
      "Number of requests: 100\n",
      "Total response time: 5.974\n",
      "Throughput (requests per second): 16.738\n",
      "Peak response time: 0.794\n",
      "Mean response time: 0.446\n",
      "99 percentile: 0.744\n",
      "add 4 workers\n",
      "------------------\n",
      "Algo: biasedmf, Workers: 8\n",
      "Number of requests: 100\n",
      "Total response time: 5.667\n",
      "Throughput (requests per second): 17.646\n",
      "Peak response time: 0.511\n",
      "Mean response time: 0.437\n",
      "99 percentile: 0.508\n",
      "add 4 workers\n",
      "------------------\n",
      "Algo: biasedmf, Workers: 12\n",
      "Number of requests: 100\n",
      "Total response time: 5.498\n",
      "Throughput (requests per second): 18.188\n",
      "Peak response time: 0.511\n",
      "Mean response time: 0.424\n",
      "99 percentile: 0.476\n",
      "add 4 workers\n",
      "------------------\n",
      "Algo: biasedmf, Workers: 16\n",
      "Number of requests: 100\n",
      "Total response time: 5.511\n",
      "Throughput (requests per second): 18.145\n",
      "Peak response time: 0.475\n",
      "Mean response time: 0.426\n",
      "99 percentile: 0.473\n",
      "add 8 workers\n",
      "------------------\n",
      "Algo: biasedmf, Workers: 24\n",
      "Number of requests: 100\n",
      "Total response time: 5.65\n",
      "Throughput (requests per second): 17.698\n",
      "Peak response time: 0.493\n",
      "Mean response time: 0.435\n",
      "99 percentile: 0.492\n",
      "------------------\n",
      "*******************************************************\n",
      "Algo: itemitem, Workers: 1\n",
      "Number of requests: 100\n",
      "Total response time: 5.082\n",
      "Throughput (requests per second): 19.678\n",
      "Peak response time: 0.55\n",
      "Mean response time: 0.392\n",
      "99 percentile: 0.538\n",
      "add 1 workers\n",
      "------------------\n",
      "Algo: itemitem, Workers: 2\n",
      "Number of requests: 100\n",
      "Total response time: 5.052\n",
      "Throughput (requests per second): 19.795\n",
      "Peak response time: 0.52\n",
      "Mean response time: 0.39\n",
      "99 percentile: 0.517\n",
      "add 2 workers\n",
      "------------------\n",
      "Algo: itemitem, Workers: 4\n",
      "Number of requests: 100\n",
      "Total response time: 4.932\n",
      "Throughput (requests per second): 20.277\n",
      "Peak response time: 0.499\n",
      "Mean response time: 0.381\n",
      "99 percentile: 0.471\n",
      "add 4 workers\n",
      "------------------\n",
      "Algo: itemitem, Workers: 8\n",
      "Number of requests: 100\n",
      "Total response time: 5.091\n",
      "Throughput (requests per second): 19.642\n",
      "Peak response time: 0.558\n",
      "Mean response time: 0.393\n",
      "99 percentile: 0.533\n",
      "add 4 workers\n",
      "------------------\n",
      "Algo: itemitem, Workers: 12\n",
      "Number of requests: 100\n",
      "Total response time: 5.397\n",
      "Throughput (requests per second): 18.529\n",
      "Peak response time: 0.608\n",
      "Mean response time: 0.417\n",
      "99 percentile: 0.606\n",
      "add 4 workers\n",
      "------------------\n",
      "Algo: itemitem, Workers: 16\n",
      "Number of requests: 100\n",
      "Total response time: 5.455\n",
      "Throughput (requests per second): 18.332\n",
      "Peak response time: 0.594\n",
      "Mean response time: 0.419\n",
      "99 percentile: 0.576\n",
      "add 8 workers\n",
      "------------------\n",
      "Algo: itemitem, Workers: 24\n",
      "Number of requests: 100\n",
      "Total response time: 5.567\n",
      "Throughput (requests per second): 17.962\n",
      "Peak response time: 0.649\n",
      "Mean response time: 0.429\n",
      "99 percentile: 0.616\n",
      "------------------\n",
      "*******************************************************\n"
     ]
    }
   ],
   "source": [
    "for current_algo in linear_speedup_algos:\n",
    "    i = 0\n",
    "    throughputs = []\n",
    "    remove_workers(3) # reduce from 4 workers to 1\n",
    "    for num_workers in workers_config:\n",
    "        print(f'Algo: {current_algo}, Workers: {num_workers}')\n",
    "        warm_up(current_algo, num_workers, display_logs=False)\n",
    "        file_name = f'preds_{current_algo}_parallel_threads_8_workers_{num_workers}_num_req_{num_requests}.csv'\n",
    "        call_server(file_name)\n",
    "        if (num_workers != workers_config[-1]):\n",
    "            print(f'add {inc_config[i]} workers')\n",
    "            add_workers(inc_config[i])\n",
    "        i += 1\n",
    "        print('------------------')\n",
    "    throughput_file_name_workers = f'throughput_single_multiple_workers_algo_{current_algo}.csv'\n",
    "    np.savetxt(throughput_file_name_workers, throughputs , delimiter=',')\n",
    "    remove_workers(workers_config[-1] - 4) # remove workers to get only 4 (default config)\n",
    "    print('*******************************************************')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Throughput by number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throughput_file_name_workers = 'throughput_single_multiple_workers.csv'\n",
    "# np.savetxt(throughput_file_name_workers, throughputs , delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throughputs_workers_from_file = np.genfromtxt(throughput_file_name_workers, delimiter=',')\n",
    "# workers = [1, 2, 4, 8, 16]\n",
    "# y_pos = np.arange(len(throughputs_workers_from_file))\n",
    "\n",
    "# plt.bar(y_pos, throughputs_workers_from_file, align='center', alpha=0.5)\n",
    "# plt.xticks(y_pos, workers)\n",
    "# plt.ylabel('Throughput')\n",
    "# plt.title('Throughput by workers')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lenskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "from binpickle import BinPickleFile\n",
    "from pathlib import Path\n",
    "\n",
    "directory_path = 'models'\n",
    "\n",
    "def exists_model_file(algo):\n",
    "    full_file_name = Path(directory_path) / algo\n",
    "    if full_file_name.exists():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def load_for_shared_mem(file_name):\n",
    "    full_file_name = Path(directory_path) / file_name\n",
    "\n",
    "    binpickle_file = BinPickleFile(full_file_name, direct=True)\n",
    "    model = binpickle_file.load()\n",
    "    return model\n",
    "\n",
    "def get_predictions_from_model(model, user, items):\n",
    "    try:\n",
    "        results = []\n",
    "        df_preds = model.predict_for_user(user, items)\n",
    "        for index, value in df_preds.iteritems():\n",
    "            if not math.isnan(value):\n",
    "                results.append({'item': index, 'score': value})\n",
    "        return results\n",
    "    except:\n",
    "        print(f\"Unexpected preds error for user: {user}, with items: {items}. Error: {sys.exc_info()[0]}\")\n",
    "        raise\n",
    "        \n",
    "\n",
    "# Predictions    \n",
    "async def get_preds_threads_lkpy(num_sem, model, file_name=None, add_throughput=False):\n",
    "    times = []\n",
    "    sem = asyncio.Semaphore(num_sem)\n",
    "    tasks = []    \n",
    "    num_requests = len(n_rand_users)\n",
    "    print(f'Number of requests: {num_requests}')\n",
    "    start_preds = perf_counter()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in n_rand_users.iterrows():\n",
    "            task = asyncio.ensure_future(get_user_preds_with_threads_lkpy(row['user'], items, session, sem, times, model))\n",
    "            tasks.append(task)         \n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        time_taken_all = perf_counter() - start_preds\n",
    "        print_stats(times, time_taken_all, num_requests)\n",
    "        \n",
    "        if file_name != None and file_name != '':\n",
    "            if os.path.exists(file_name):\n",
    "                os.remove(file_name)\n",
    "            np.asarray(times)\n",
    "            np.savetxt(file_name, times, delimiter=',')\n",
    "        \n",
    "        if add_throughput:\n",
    "            throughputs.append(num_requests / time_taken_all)\n",
    "\n",
    "async def get_user_preds_with_threads_lkpy(user, items, session, sem, times, model):\n",
    "    async with sem:  # semaphore limits num of simultaneous downloads\n",
    "        return await get_user_preds_threads_lkpy(user, items, session, times, model)        \n",
    "        \n",
    "async def get_user_preds_threads_lkpy(user, items, session, times, model):\n",
    "    try:\n",
    "        start = perf_counter()\n",
    "        results = []\n",
    "        df_preds = model.predict_for_user(user, items.split(','))\n",
    "        for index, value in df_preds.iteritems():\n",
    "            if not math.isnan(value):\n",
    "                results.append({'item': index, 'score': value})\n",
    "                \n",
    "        time_taken = perf_counter() - start\n",
    "        times.append(time_taken)\n",
    "        return results\n",
    "    except:\n",
    "        print(f\"Unexpected preds error for user: {user}, with items: {items}. Error: {sys.exc_info()[0]}\")\n",
    "        raise        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/anaconda3/lib/python3.7/site-packages/fastparquet/encoding.py:222: NumbaDeprecationWarning: \u001b[1mThe 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\u001b[0m\n",
      "  Numpy8 = numba.jitclass(spec8)(NumpyIO)\n",
      "/Users/carlos/anaconda3/lib/python3.7/site-packages/fastparquet/encoding.py:224: NumbaDeprecationWarning: \u001b[1mThe 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\u001b[0m\n",
      "  Numpy32 = numba.jitclass(spec32)(NumpyIO)\n",
      "/Users/carlos/anaconda3/lib/python3.7/site-packages/fastparquet/dataframe.py:5: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import CategoricalIndex, RangeIndex, Index, MultiIndex\n"
     ]
    }
   ],
   "source": [
    "import train_save_model\n",
    "lk_recserver_algos = reader.get_value('lk_recserver_algos')\n",
    "lk_recserver_algos_not_created = []\n",
    "for a in lk_recserver_algos:\n",
    "    if not exists_model_file(f'{a}.bpk'):\n",
    "        lk_recserver_algos_not_created.append(a)\n",
    "if len(lk_recserver_algos_not_created) > 0:\n",
    "    train_save_model.save_models(lk_recserver_algos_not_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo: bias\n",
      "Lenskit performance:\n",
      "Number of requests: 100\n",
      "Total response time: 0.441\n",
      "Throughput (requests per second): 226.514\n",
      "Peak response time: 0.237\n",
      "Mean response time: 0.004\n",
      "99 percentile: 0.006\n",
      "------------------\n",
      "Recommendation server performance:\n",
      "Number of requests: 100\n",
      "Total response time: 4.504\n",
      "Throughput (requests per second): 22.203\n",
      "Peak response time: 0.412\n",
      "Mean response time: 0.346\n",
      "99 percentile: 0.408\n",
      "*******************************************************\n",
      "Algo: biasedmf\n",
      "Lenskit performance:\n",
      "Number of requests: 100\n",
      "Total response time: 0.264\n",
      "Throughput (requests per second): 378.559\n",
      "Peak response time: 0.005\n",
      "Mean response time: 0.002\n",
      "99 percentile: 0.005\n",
      "------------------\n",
      "Recommendation server performance:\n",
      "Number of requests: 100\n",
      "Total response time: 5.978\n",
      "Throughput (requests per second): 16.727\n",
      "Peak response time: 0.552\n",
      "Mean response time: 0.463\n",
      "99 percentile: 0.529\n",
      "*******************************************************\n",
      "Algo: itemitem\n",
      "Lenskit performance:\n",
      "Number of requests: 100\n",
      "Total response time: 0.063\n",
      "Throughput (requests per second): 1577.993\n",
      "Peak response time: 0.001\n",
      "Mean response time: 0.0\n",
      "99 percentile: 0.001\n",
      "------------------\n",
      "Recommendation server performance:\n",
      "Number of requests: 100\n",
      "Total response time: 5.347\n",
      "Throughput (requests per second): 18.702\n",
      "Peak response time: 0.567\n",
      "Mean response time: 0.411\n",
      "99 percentile: 0.551\n",
      "*******************************************************\n"
     ]
    }
   ],
   "source": [
    "for lk_recserver_algo in lk_recserver_algos:\n",
    "    print(f'Algo: {lk_recserver_algo}')\n",
    "    print('Lenskit performance:')\n",
    "    model = load_for_shared_mem(f'{lk_recserver_algo}.bpk')\n",
    "    file_name = f'lkpy_parallel_threads_8_{lk_recserver_algo}__num_req_{num_requests}.csv'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_preds_threads_lkpy(8, model, file_name))\n",
    "    loop.run_until_complete(future)\n",
    "    #plot_numbers(file_name)\n",
    "    #hist_numbers(file_name)\n",
    "    print('------------------')    \n",
    "    warm_up(lk_recserver_algo, 8, False)\n",
    "    print('Recommendation server performance:')\n",
    "    file_name = f'preds_{lk_recserver_algo}_parallel_threads_8_workers_4_num_req_{num_requests}.csv'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_preds_sem(8, lk_recserver_algo, file_name, True))\n",
    "    loop.run_until_complete(future)\n",
    "    #plot_numbers(file_name)\n",
    "    #hist_numbers(file_name)\n",
    "    print('*******************************************************')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
