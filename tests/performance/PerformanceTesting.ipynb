{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install aiohttp\n",
    "# %pip install mysql-connector-python\n",
    "# %pip install nest_asyncio\n",
    "# %pip install lenskit --upgrade\n",
    "# %pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n",
    "import urllib\n",
    "from pandas.io import sql\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import requests\n",
    "from time import perf_counter\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigReader:\n",
    "    def get_value(self, key):\n",
    "        with open('config.json') as json_data_file:\n",
    "            data = json.load(json_data_file)\n",
    "        return data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbManager:\n",
    "    def __init__(self):\n",
    "        reader = ConfigReader()\n",
    "        db_connection = reader.get_value(\"db_connection\")        \n",
    "        self.conn_string = '{db_engine}{connector}://{user}:{password}@{server}/{database}?port={port}'.format(\n",
    "            db_engine=db_connection['db_engine'],\n",
    "            connector=db_connection['connector'],\n",
    "            user=db_connection['user'],\n",
    "            password=db_connection['password'],\n",
    "            server=db_connection['server'],\n",
    "            database=db_connection['database'],\n",
    "            port=db_connection['port'])\n",
    "\n",
    "    def get_users(self):\n",
    "        return sql.read_sql(\"SELECT distinct user FROM rating;\", create_engine(self.conn_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get random users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rand_users = num_requests = 10\n",
    "dbManager = DbManager()\n",
    "db_users = dbManager.get_users()\n",
    "n_rand_users = db_users.sample(n=n_rand_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test recommendation endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ConfigReader()\n",
    "base_url = reader.get_value(\"rec_server_baese_url\")\n",
    "n_recs = reader.get_value(\"n_recs\")\n",
    "items = reader.get_value(\"items\")\n",
    "pred_algos = reader.get_value(\"pred_algos\")\n",
    "rec_algos = reader.get_value(\"rec_algos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semaphore performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "throughputs = []\n",
    "\n",
    "def print_stats(times, time_taken_all, num_requests):\n",
    "    print(f'Total response time: {round(time_taken_all, 3)}')\n",
    "    print(f'Throughput (requests per second): {round(num_requests / time_taken_all, 3)}')\n",
    "    print(f'Peak response time: {max(round(times, 3))}')\n",
    "    print(f'Mean response time: {round(np.mean(times), 3)}')\n",
    "    print(f'99 percentile: {round(np.quantile(times, 0.99), 3)}')\n",
    "\n",
    "def plot_numbers(file_name):\n",
    "    resp_time_per_request = np.genfromtxt(file_name, delimiter=',')\n",
    "    plt.plot(resp_time_per_request)\n",
    "    plt.show()\n",
    "    \n",
    "def hist_numbers(file_name):\n",
    "    resp_time_per_request = np.genfromtxt(file_name, delimiter=',')\n",
    "    plt.hist(resp_time_per_request, bins='auto')\n",
    "    plt.show()\n",
    "\n",
    "# Predictions    \n",
    "async def get_preds_sem(num_sem, algo_pred, file_name=None, add_throughput=False):\n",
    "    times = []\n",
    "    sem = asyncio.Semaphore(num_sem)\n",
    "    tasks = []    \n",
    "    num_requests = len(n_rand_users)\n",
    "    print(f'Number of requests: {num_requests}')\n",
    "    start_preds = perf_counter()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in n_rand_users.iterrows():\n",
    "            task = asyncio.ensure_future(get_user_preds_with_sem(row['user'], algo_pred, items, session, sem, times))\n",
    "            tasks.append(task)         \n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        time_taken_all = perf_counter() - start_preds\n",
    "        print_stats(times, time_taken_all, num_requests)\n",
    "        \n",
    "        if file_name != None and file_name != '':\n",
    "            if os.path.exists(file_name):\n",
    "                os.remove(file_name)\n",
    "            np.savetxt(file_name, times, delimiter=',')\n",
    "        \n",
    "        if add_throughput:\n",
    "            throughputs.append(num_requests / time_taken_all)\n",
    "\n",
    "async def get_user_preds_with_sem(user, algo, items, session, sem, times):\n",
    "    async with sem:  # semaphore limits num of simultaneous downloads\n",
    "        return await get_user_preds_sem(user, algo, items, session, times)        \n",
    "        \n",
    "async def get_user_preds_sem(user, algo, items, session, times):\n",
    "    url = f'{base_url}/algorithms/{algo}/predictions?user_id={user}&items={items}'\n",
    "    start = perf_counter()\n",
    "    async with session.get(url) as resp:\n",
    "        data = await resp.json()    \n",
    "        time_taken = perf_counter() - start\n",
    "        times.append(time_taken)\n",
    "        \n",
    "# Recommendations\n",
    "async def get_recs_sem(num_sem, algo_rec, file_name=None, add_throughput=False):\n",
    "    times = []\n",
    "    sem = asyncio.Semaphore(num_sem)\n",
    "    tasks = []\n",
    "    num_requests = len(n_rand_users)\n",
    "    print(f'Number of requests: {num_requests}')\n",
    "    start_preds = perf_counter()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in n_rand_users.iterrows():\n",
    "            task = asyncio.ensure_future(get_user_recs_with_sem(row['user'], algo_rec, n_recs, session, sem, times))\n",
    "            tasks.append(task)         \n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        time_taken_all = perf_counter() - start_preds\n",
    "        print_stats(times, time_taken_all, num_requests)\n",
    "        \n",
    "        if file_name != None and file_name != '':\n",
    "            if os.path.exists(file_name):\n",
    "                os.remove(file_name)\n",
    "            np.savetxt(file_name, times, delimiter=',')\n",
    "        \n",
    "        if add_throughput:\n",
    "            throughputs.append(num_requests / time_taken_all)\n",
    "\n",
    "async def get_user_recs_with_sem(user, algo, n_recs, session, sem, times):\n",
    "    async with sem:  # semaphore limits num of simultaneous downloads\n",
    "        return await get_user_preds_sem(user, algo, n_recs, session, times)        \n",
    "        \n",
    "async def get_user_recs_sem(user, algo, n_recs, session, times):\n",
    "    url = f'{base_url}/algorithms/{algo}/recommendations?user_id={user}&num_recs={n_recs}'\n",
    "    start = perf_counter()\n",
    "    async with session.get(url) as resp:\n",
    "        data = await resp.json()    \n",
    "        time_taken = perf_counter() - start\n",
    "        times.append(time_taken)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def warm_up_async(current_algo=None, num_workers=24, display_logs=True):\n",
    "    warm_up_user = 1\n",
    "    times = []\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for algo in pred_algos:\n",
    "            if current_algo is None or algo == current_algo:\n",
    "                for w in range(num_workers * 2):\n",
    "                    if display_logs:\n",
    "                        print(f'Calling {algo}. Worker number: {w + 1}')\n",
    "                    task = asyncio.ensure_future(get_user_preds_sem(warm_up_user, algo, items, session, times))\n",
    "                    tasks.append(task)\n",
    "        responses = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warm_up(current_algo=None, num_workers=24, display_logs=True):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(warm_up_async(current_algo, num_workers, display_logs))\n",
    "    loop.run_until_complete(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling bias. Worker number: 1\n",
      "Calling bias. Worker number: 2\n",
      "Calling bias. Worker number: 3\n",
      "Calling bias. Worker number: 4\n",
      "Calling bias. Worker number: 5\n",
      "Calling bias. Worker number: 6\n",
      "Calling bias. Worker number: 7\n",
      "Calling bias. Worker number: 8\n",
      "Calling itemitem. Worker number: 1\n",
      "Calling itemitem. Worker number: 2\n",
      "Calling itemitem. Worker number: 3\n",
      "Calling itemitem. Worker number: 4\n",
      "Calling itemitem. Worker number: 5\n",
      "Calling itemitem. Worker number: 6\n",
      "Calling itemitem. Worker number: 7\n",
      "Calling itemitem. Worker number: 8\n",
      "Calling useruser. Worker number: 1\n",
      "Calling useruser. Worker number: 2\n",
      "Calling useruser. Worker number: 3\n",
      "Calling useruser. Worker number: 4\n",
      "Calling useruser. Worker number: 5\n",
      "Calling useruser. Worker number: 6\n",
      "Calling useruser. Worker number: 7\n",
      "Calling useruser. Worker number: 8\n",
      "Calling biasedmf. Worker number: 1\n",
      "Calling biasedmf. Worker number: 2\n",
      "Calling biasedmf. Worker number: 3\n",
      "Calling biasedmf. Worker number: 4\n",
      "Calling biasedmf. Worker number: 5\n",
      "Calling biasedmf. Worker number: 6\n",
      "Calling biasedmf. Worker number: 7\n",
      "Calling biasedmf. Worker number: 8\n",
      "Calling implicitmf. Worker number: 1\n",
      "Calling implicitmf. Worker number: 2\n",
      "Calling implicitmf. Worker number: 3\n",
      "Calling implicitmf. Worker number: 4\n",
      "Calling implicitmf. Worker number: 5\n",
      "Calling implicitmf. Worker number: 6\n",
      "Calling implicitmf. Worker number: 7\n",
      "Calling implicitmf. Worker number: 8\n",
      "Calling funksvd. Worker number: 1\n",
      "Calling funksvd. Worker number: 2\n",
      "Calling funksvd. Worker number: 3\n",
      "Calling funksvd. Worker number: 4\n",
      "Calling funksvd. Worker number: 5\n",
      "Calling funksvd. Worker number: 6\n",
      "Calling funksvd. Worker number: 7\n",
      "Calling funksvd. Worker number: 8\n"
     ]
    }
   ],
   "source": [
    "warm_up(None, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gunicorn methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def get_gunicorn_master_pid():\n",
    "    proc1 = subprocess.Popen(['ps', 'ax'], stdout=subprocess.PIPE)\n",
    "    proc2 = subprocess.Popen(['grep', 'gunicorn'], stdin=proc1.stdout,\n",
    "                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    proc1.stdout.close() # Allow proc1 to receive a SIGPIPE if proc2 exits.\n",
    "    out, err = proc2.communicate()\n",
    "    master_id = out[:6].decode('utf-8').replace(' ', '')\n",
    "    return master_id\n",
    "\n",
    "def add_workers(n):\n",
    "    master_id = get_gunicorn_master_pid()\n",
    "    for i in range(n):\n",
    "        os.system(f\"kill -s TTIN {master_id}\")\n",
    "        \n",
    "def remove_workers(n):\n",
    "    master_id = get_gunicorn_master_pid()\n",
    "    for i in range(n):\n",
    "        os.system(f\"kill -s TTOU {master_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call predict and recommend from server for canonical config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions for different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: bias\n",
      "Number of requests: 10\n",
      "Total response time: 0.46764893199999946\n",
      "Throughput (requests per second): 21.383562146144303\n",
      "Peak response time: 0.3520947149999998\n",
      "Mean response time: 0.2591534149999998\n",
      "99 percentile: 0.35084565143999985\n",
      "---------------------\n",
      "\n",
      "Algorithm: itemitem\n",
      "Number of requests: 10\n",
      "Total response time: 7.889670232\n",
      "Throughput (requests per second): 1.267480098146642\n",
      "Peak response time: 6.152391770000001\n",
      "Mean response time: 4.440071012000001\n",
      "99 percentile: 6.148081142600001\n",
      "---------------------\n",
      "\n",
      "Algorithm: useruser\n",
      "Number of requests: 10\n",
      "Total response time: 0.756184753000003\n",
      "Throughput (requests per second): 13.224281447526039\n",
      "Peak response time: 0.6185452230000017\n",
      "Mean response time: 0.46188736219999954\n",
      "99 percentile: 0.6181211909700016\n",
      "---------------------\n",
      "\n",
      "Algorithm: biasedmf\n",
      "Number of requests: 10\n",
      "Total response time: 0.8531932320000024\n",
      "Throughput (requests per second): 11.72067431495984\n",
      "Peak response time: 0.6406673279999993\n",
      "Mean response time: 0.4669342068000006\n",
      "99 percentile: 0.6361894488299993\n",
      "---------------------\n",
      "\n",
      "Algorithm: implicitmf\n",
      "Number of requests: 10\n",
      "Total response time: 0.5873904370000034\n",
      "Throughput (requests per second): 17.024451489325052\n",
      "Peak response time: 0.4383506550000007\n",
      "Mean response time: 0.3370392924000001\n",
      "99 percentile: 0.43811258538000086\n",
      "---------------------\n",
      "\n",
      "Algorithm: funksvd\n",
      "Number of requests: 10\n",
      "Total response time: 0.6102753870000015\n",
      "Throughput (requests per second): 16.38604507574541\n",
      "Peak response time: 0.4347603479999975\n",
      "Mean response time: 0.32826894089999925\n",
      "99 percentile: 0.43406081156999743\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for algo in pred_algos:\n",
    "    file_name = f'preds_{algo}_parallel_threads_8_workers_4_num_req_{num_requests}.csv'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    print(f'Algorithm: {algo}')\n",
    "    future = asyncio.ensure_future(get_preds_sem(8, algo, file_name, True))\n",
    "    loop.run_until_complete(future)\n",
    "#    plot_numbers(file_name)\n",
    "#    hist_numbers(file_name)\n",
    "    print('---------------------')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of requests: 10\n",
      "Total response time: 0.06585761599999529\n",
      "Throughput (requests per second): 151.8427268913092\n",
      "Peak response time: 0.05490221000000162\n",
      "Mean response time: 0.03665522989999985\n",
      "99 percentile: 0.05393225264000151\n"
     ]
    }
   ],
   "source": [
    "algo_rec = 'popular'\n",
    "file_name = f'recs_{algo_rec}_parallel_threads_8_workers_4_num_req_{num_requests}.csv'\n",
    "loop = asyncio.get_event_loop()\n",
    "future = asyncio.ensure_future(get_recs_sem(8, algo_rec, file_name))\n",
    "loop.run_until_complete(future)\n",
    "#plot_numbers(file_name)\n",
    "#hist_numbers(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speedup Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughputs = []\n",
    "linear_speedup_algos = ['biasedmf', 'itemitem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_server(file_name):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_preds_sem(8, current_algo, file_name, True))\n",
    "    loop.run_until_complete(future)\n",
    "#    plot_numbers(file_name)\n",
    "#    hist_numbers(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_config = [1, 2, 4] #, 8, 12, 16, 24]\n",
    "inc_config = [1, 2, 4] #, 4, 4, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo: biasedmf, Workers: 1\n",
      "Number of requests: 10\n",
      "Total response time: 0.8080285810000021\n",
      "Throughput (requests per second): 12.375799860475448\n",
      "Peak response time: 0.5996992959999972\n",
      "Mean response time: 0.44603972229999866\n",
      "99 percentile: 0.5961555139599971\n",
      "add 1 workers\n",
      "------------------\n",
      "Algo: biasedmf, Workers: 2\n",
      "Number of requests: 10\n",
      "Total response time: 0.907169863\n",
      "Throughput (requests per second): 11.023293881181324\n",
      "Peak response time: 0.6838896860000006\n",
      "Mean response time: 0.5288647282000014\n",
      "99 percentile: 0.6832141591400004\n",
      "add 2 workers\n",
      "------------------\n",
      "Algo: biasedmf, Workers: 4\n",
      "Number of requests: 10\n",
      "Total response time: 0.7864023079999996\n",
      "Throughput (requests per second): 12.716137654061928\n",
      "Peak response time: 0.5642168560000016\n",
      "Mean response time: 0.43372251320000077\n",
      "99 percentile: 0.5634535576300016\n",
      "------------------\n",
      "*******************************************************\n",
      "Algo: itemitem, Workers: 1\n",
      "Number of requests: 10\n",
      "Total response time: 5.922881746000002\n",
      "Throughput (requests per second): 1.688367323347873\n",
      "Peak response time: 4.447846038000002\n",
      "Mean response time: 3.334342225300001\n",
      "99 percentile: 4.443810454380001\n",
      "add 1 workers\n",
      "------------------\n",
      "Algo: itemitem, Workers: 2\n",
      "Number of requests: 10\n",
      "Total response time: 6.214759739000002\n",
      "Throughput (requests per second): 1.6090726624950862\n",
      "Peak response time: 4.593227706999997\n",
      "Mean response time: 3.5355812694\n",
      "99 percentile: 4.5921214024299974\n",
      "add 2 workers\n",
      "------------------\n",
      "Algo: itemitem, Workers: 4\n",
      "Number of requests: 10\n",
      "Total response time: 6.6260329019999915\n",
      "Throughput (requests per second): 1.509198663499183\n",
      "Peak response time: 4.786311519999998\n",
      "Mean response time: 3.665091153999999\n",
      "99 percentile: 4.774770932499998\n",
      "------------------\n",
      "*******************************************************\n"
     ]
    }
   ],
   "source": [
    "for current_algo in linear_speedup_algos:\n",
    "    i = 0\n",
    "    throughputs = []\n",
    "    remove_workers(3) # reduce from 4 workers to 1\n",
    "    for num_workers in workers_config:\n",
    "        print(f'Algo: {current_algo}, Workers: {num_workers}')\n",
    "        warm_up(current_algo, num_workers, display_logs=False)\n",
    "        file_name = f'preds_{current_algo}_parallel_threads_8_workers_{num_workers}_num_req_{num_requests}.csv'\n",
    "        call_server(file_name)\n",
    "        if (num_workers != workers_config[-1]):\n",
    "            print(f'add {inc_config[i]} workers')\n",
    "            add_workers(inc_config[i])\n",
    "        i += 1\n",
    "        print('------------------')\n",
    "    throughput_file_name_workers = f'throughput_single_multiple_workers_algo_{current_algo}.csv'\n",
    "    np.savetxt(throughput_file_name_workers, throughputs , delimiter=',')\n",
    "    remove_workers(workers_config[-1] - 4) # remove workers to get only 4 (default config)\n",
    "    print('*******************************************************')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Throughput by number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throughput_file_name_workers = 'throughput_single_multiple_workers.csv'\n",
    "# np.savetxt(throughput_file_name_workers, throughputs , delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throughputs_workers_from_file = np.genfromtxt(throughput_file_name_workers, delimiter=',')\n",
    "# workers = [1, 2, 4, 8, 16]\n",
    "# y_pos = np.arange(len(throughputs_workers_from_file))\n",
    "\n",
    "# plt.bar(y_pos, throughputs_workers_from_file, align='center', alpha=0.5)\n",
    "# plt.xticks(y_pos, workers)\n",
    "# plt.ylabel('Throughput')\n",
    "# plt.title('Throughput by workers')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lenskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "from binpickle import BinPickleFile\n",
    "from pathlib import Path\n",
    "\n",
    "directory_path = 'models'\n",
    "algo_pred_lkpy = 'bias.bpk'\n",
    "\n",
    "def load_for_shared_mem(file_name):\n",
    "    full_file_name = Path(directory_path) / file_name\n",
    "\n",
    "    binpickle_file = BinPickleFile(full_file_name, direct=True)\n",
    "    model = binpickle_file.load()\n",
    "    return model\n",
    "\n",
    "def get_predictions_from_model(model, user, items):\n",
    "    try:\n",
    "        results = []\n",
    "        df_preds = model.predict_for_user(user, items)\n",
    "        for index, value in df_preds.iteritems():\n",
    "            if not math.isnan(value):\n",
    "                results.append({'item': index, 'score': value})\n",
    "        return results\n",
    "    except:\n",
    "        print(f\"Unexpected preds error for user: {user}, with items: {items}. Error: {sys.exc_info()[0]}\")\n",
    "        raise\n",
    "        \n",
    "\n",
    "# Predictions    \n",
    "async def get_preds_threads_lkpy(num_sem, model, file_name=None, add_throughput=False):\n",
    "    times = []\n",
    "    sem = asyncio.Semaphore(num_sem)\n",
    "    tasks = []    \n",
    "    num_requests = len(n_rand_users)\n",
    "    print(f'Number of requests: {num_requests}')\n",
    "    start_preds = perf_counter()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in n_rand_users.iterrows():\n",
    "            task = asyncio.ensure_future(get_user_preds_with_threads_lkpy(row['user'], algo_pred_lkpy, items, session, sem, times, model))\n",
    "            tasks.append(task)         \n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        time_taken_all = perf_counter() - start_preds\n",
    "        print_stats(times, time_taken_all, num_requests)\n",
    "        \n",
    "        if file_name != None and file_name != '':\n",
    "            if os.path.exists(file_name):\n",
    "                os.remove(file_name)\n",
    "            np.asarray(times)\n",
    "            np.savetxt(file_name, times, delimiter=',')\n",
    "        \n",
    "        if add_throughput:\n",
    "            throughputs.append(num_requests / time_taken_all)\n",
    "\n",
    "async def get_user_preds_with_threads_lkpy(user, algo, items, session, sem, times, model):\n",
    "    async with sem:  # semaphore limits num of simultaneous downloads\n",
    "        return await get_user_preds_threads_lkpy(user, algo, items, session, times, model)        \n",
    "        \n",
    "async def get_user_preds_threads_lkpy(user, algo, items, session, times, model):\n",
    "    try:\n",
    "        start = perf_counter()\n",
    "        results = []\n",
    "        df_preds = model.predict_for_user(user, items.split(','))\n",
    "        for index, value in df_preds.iteritems():\n",
    "            if not math.isnan(value):\n",
    "                results.append({'item': index, 'score': value})\n",
    "                \n",
    "        time_taken = perf_counter() - start\n",
    "        times.append(time_taken)\n",
    "        return results\n",
    "    except:\n",
    "        print(f\"Unexpected preds error for user: {user}, with items: {items}. Error: {sys.exc_info()[0]}\")\n",
    "        raise        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/anaconda3/lib/python3.7/site-packages/fastparquet/encoding.py:222: NumbaDeprecationWarning: \u001b[1mThe 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\u001b[0m\n",
      "  Numpy8 = numba.jitclass(spec8)(NumpyIO)\n",
      "/Users/carlos/anaconda3/lib/python3.7/site-packages/fastparquet/encoding.py:224: NumbaDeprecationWarning: \u001b[1mThe 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\u001b[0m\n",
      "  Numpy32 = numba.jitclass(spec32)(NumpyIO)\n",
      "/Users/carlos/anaconda3/lib/python3.7/site-packages/fastparquet/dataframe.py:5: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import CategoricalIndex, RangeIndex, Index, MultiIndex\n"
     ]
    }
   ],
   "source": [
    "import train_save_model\n",
    "algos = \"bias, biasedmf\" #, itemitem\"\n",
    "train_save_model.save_models(algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo: bias\n",
      "Lenskit performance:\n",
      "Number of requests: 10\n",
      "Total response time: 0.42222850200001005\n",
      "Throughput (requests per second): 23.683858272551582\n",
      "Peak response time: 0.28807621900000413\n",
      "Mean response time: 0.0418654016000005\n",
      "99 percentile: 0.26371981015000395\n",
      "------------------\n",
      "Recommendation server performance:\n",
      "Number of requests: 10\n",
      "Total response time: 0.5226947139999965\n",
      "Throughput (requests per second): 19.131626420082885\n",
      "Peak response time: 0.37786488599999757\n",
      "Mean response time: 0.2874253101999983\n",
      "99 percentile: 0.37683360965999696\n",
      "*******************************************************\n",
      "Algo: biasedmf\n",
      "Lenskit performance:\n",
      "Number of requests: 10\n",
      "Total response time: 0.12905851900001153\n",
      "Throughput (requests per second): 77.4842302351161\n",
      "Peak response time: 0.022408049000006258\n",
      "Mean response time: 0.012666509700000005\n",
      "99 percentile: 0.021493129970005926\n",
      "------------------\n",
      "Recommendation server performance:\n",
      "Number of requests: 10\n",
      "Total response time: 0.4755798810000016\n",
      "Throughput (requests per second): 21.02696181969053\n",
      "Peak response time: 0.3365659780000101\n",
      "Mean response time: 0.26018898930000206\n",
      "99 percentile: 0.3361042195900093\n",
      "*******************************************************\n"
     ]
    }
   ],
   "source": [
    "lk_recserver_algos = ['bias', 'biasedmf'] #, 'itemitem']\n",
    "for lk_recserver_algo in lk_recserver_algos:\n",
    "    print(f'Algo: {lk_recserver_algo}')\n",
    "    print('Lenskit performance:')\n",
    "    model = load_for_shared_mem(f'{lk_recserver_algo}.bpk')\n",
    "    file_name = f'lkpy_parallel_threads_8_{lk_recserver_algo}__num_req_{num_requests}.csv'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_preds_threads_lkpy(8, model, file_name))\n",
    "    loop.run_until_complete(future)\n",
    "    #plot_numbers(file_name)\n",
    "    #hist_numbers(file_name)\n",
    "    print('------------------')    \n",
    "    warm_up(algo, 8, False)\n",
    "    print('Recommendation server performance:')\n",
    "    file_name = f'preds_{lk_recserver_algo}_parallel_threads_8_workers_4_num_req_{num_requests}.csv'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_preds_sem(8, algo, file_name, True))\n",
    "    loop.run_until_complete(future)\n",
    "    #plot_numbers(file_name)\n",
    "    #hist_numbers(file_name)\n",
    "    print('*******************************************************')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
