{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install aiohttp\n",
    "# %pip install mysql-connector-python\n",
    "# %pip install nest_asyncio\n",
    "# %pip install lenskit --upgrade\n",
    "# %pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n",
    "import urllib\n",
    "from pandas.io import sql\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import requests\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigReader:\n",
    "    def get_value(self, key):\n",
    "        with open('config.json') as json_data_file:\n",
    "            data = json.load(json_data_file)\n",
    "        return data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbManager:\n",
    "    def __init__(self):\n",
    "        reader = ConfigReader()\n",
    "        db_connection = reader.get_value(\"db_connection\")        \n",
    "        self.conn_string = '{db_engine}{connector}://{user}:{password}@{server}/{database}'.format(\n",
    "            db_engine=db_connection['db_engine'],\n",
    "            connector=db_connection['connector'],\n",
    "            user=db_connection['user'],\n",
    "            password=db_connection['password'],\n",
    "            server=db_connection['server'],\n",
    "            database=db_connection['database'])\n",
    "\n",
    "    def get_users(self):\n",
    "        return sql.read_sql(\"SELECT distinct user FROM rating;\", create_engine(self.conn_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get random users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rand_users = num_requests = 10\n",
    "dbManager = DbManager()\n",
    "db_users = dbManager.get_users()\n",
    "n_rand_users = db_users.sample(n=n_rand_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test recommendation endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://127.0.0.1:8000'\n",
    "n_recs = 5\n",
    "items = \"10,20,30,40,50\"\n",
    "pred_algos = ['bias','itemitem','useruser','biasedmf','implicitmf','funksvd']\n",
    "rec_algos = ['popular']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semaphore performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "throughputs = []\n",
    "\n",
    "def print_stats(times, time_taken_all, num_requests):\n",
    "    print(f'Total response time: {time_taken_all}')\n",
    "    print(f'Throughput (requests per second): {num_requests / time_taken_all}')\n",
    "    print(f'Peak response time: {max(times)}')\n",
    "    print(f'Mean response time: {np.mean(times)}')\n",
    "    print(f'99 percentile: {np.quantile(times, 0.99)}')\n",
    "\n",
    "def plot_numbers(file_name):\n",
    "    resp_time_per_request = np.genfromtxt(file_name, delimiter=',')\n",
    "    plt.plot(resp_time_per_request)\n",
    "    plt.show()\n",
    "    \n",
    "def hist_numbers(file_name):\n",
    "    resp_time_per_request = np.genfromtxt(file_name, delimiter=',')\n",
    "    plt.hist(resp_time_per_request, bins='auto')\n",
    "    plt.show()\n",
    "\n",
    "# Predictions    \n",
    "async def get_preds_sem(num_sem, algo_pred, file_name=None, add_throughput=False):\n",
    "    times = []\n",
    "    sem = asyncio.Semaphore(num_sem)\n",
    "    tasks = []    \n",
    "    num_requests = len(n_rand_users)\n",
    "    print(f'Number of requests: {num_requests}')\n",
    "    start_preds = perf_counter()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in n_rand_users.iterrows():\n",
    "            task = asyncio.ensure_future(get_user_preds_with_sem(row['user'], algo_pred, items, session, sem, times))\n",
    "            tasks.append(task)         \n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        time_taken_all = perf_counter() - start_preds\n",
    "        print_stats(times, time_taken_all, num_requests)\n",
    "        \n",
    "        if file_name != None and file_name != '':\n",
    "            if os.path.exists(file_name):\n",
    "                os.remove(file_name)\n",
    "            np.savetxt(file_name, times, delimiter=',')\n",
    "        \n",
    "        if add_throughput:\n",
    "            throughputs.append(num_requests / time_taken_all)\n",
    "\n",
    "async def get_user_preds_with_sem(user, algo, items, session, sem, times):\n",
    "    async with sem:  # semaphore limits num of simultaneous downloads\n",
    "        return await get_user_preds_sem(user, algo, items, session, times)        \n",
    "        \n",
    "async def get_user_preds_sem(user, algo, items, session, times):\n",
    "    url = f'{base_url}/algorithms/{algo}/predictions?user_id={user}&items={items}'\n",
    "    start = perf_counter()\n",
    "    async with session.get(url) as resp:\n",
    "        data = await resp.json()    \n",
    "        time_taken = perf_counter() - start\n",
    "        times.append(time_taken)\n",
    "        \n",
    "# Recommendations\n",
    "async def get_recs_sem(num_sem, algo_rec, file_name=None, add_throughput=False):\n",
    "    times = []\n",
    "    sem = asyncio.Semaphore(num_sem)\n",
    "    tasks = []\n",
    "    num_requests = len(n_rand_users)\n",
    "    print(f'Number of requests: {num_requests}')\n",
    "    start_preds = perf_counter()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in n_rand_users.iterrows():\n",
    "            task = asyncio.ensure_future(get_user_recs_with_sem(row['user'], algo_rec, n_recs, session, sem, times))\n",
    "            tasks.append(task)         \n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        time_taken_all = perf_counter() - start_preds\n",
    "        print_stats(times, time_taken_all, num_requests)\n",
    "        \n",
    "        if file_name != None and file_name != '':\n",
    "            if os.path.exists(file_name):\n",
    "                os.remove(file_name)\n",
    "            np.savetxt(file_name, times, delimiter=',')\n",
    "        \n",
    "        if add_throughput:\n",
    "            throughputs.append(num_requests / time_taken_all)\n",
    "\n",
    "async def get_user_recs_with_sem(user, algo, n_recs, session, sem, times):\n",
    "    async with sem:  # semaphore limits num of simultaneous downloads\n",
    "        return await get_user_preds_sem(user, algo, n_recs, session, times)        \n",
    "        \n",
    "async def get_user_recs_sem(user, algo, n_recs, session, times):\n",
    "    url = f'{base_url}/algorithms/{algo}/recommendations?user_id={user}&num_recs={n_recs}'\n",
    "    start = perf_counter()\n",
    "    async with session.get(url) as resp:\n",
    "        data = await resp.json()    \n",
    "        time_taken = perf_counter() - start\n",
    "        times.append(time_taken)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def warm_up_async(current_algo=None, num_workers=24, display_logs=True):\n",
    "    warm_up_user = 1\n",
    "    times = []\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for algo in pred_algos:\n",
    "            if current_algo is None or algo == current_algo:\n",
    "                for w in range(num_workers * 2):\n",
    "                    if display_logs:\n",
    "                        print(f'Calling {algo}. Worker number: {w + 1}')\n",
    "                    task = asyncio.ensure_future(get_user_preds_sem(warm_up_user, algo, items, session, times))\n",
    "                    tasks.append(task)\n",
    "        responses = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warm_up(current_algo=None, num_workers=24, display_logs=True):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(warm_up_async(current_algo, num_workers, display_logs))\n",
    "    loop.run_until_complete(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Calling bias. Worker number: 1\n",
      "Calling bias. Worker number: 2\n",
      "Calling bias. Worker number: 3\n",
      "Calling bias. Worker number: 4\n",
      "Calling bias. Worker number: 5\n",
      "Calling bias. Worker number: 6\n",
      "Calling bias. Worker number: 7\n",
      "Calling bias. Worker number: 8\n",
      "None\n",
      "Calling itemitem. Worker number: 1\n",
      "Calling itemitem. Worker number: 2\n",
      "Calling itemitem. Worker number: 3\n",
      "Calling itemitem. Worker number: 4\n",
      "Calling itemitem. Worker number: 5\n",
      "Calling itemitem. Worker number: 6\n",
      "Calling itemitem. Worker number: 7\n",
      "Calling itemitem. Worker number: 8\n",
      "None\n",
      "Calling useruser. Worker number: 1\n",
      "Calling useruser. Worker number: 2\n",
      "Calling useruser. Worker number: 3\n",
      "Calling useruser. Worker number: 4\n",
      "Calling useruser. Worker number: 5\n",
      "Calling useruser. Worker number: 6\n",
      "Calling useruser. Worker number: 7\n",
      "Calling useruser. Worker number: 8\n",
      "None\n",
      "Calling biasedmf. Worker number: 1\n",
      "Calling biasedmf. Worker number: 2\n",
      "Calling biasedmf. Worker number: 3\n",
      "Calling biasedmf. Worker number: 4\n",
      "Calling biasedmf. Worker number: 5\n",
      "Calling biasedmf. Worker number: 6\n",
      "Calling biasedmf. Worker number: 7\n",
      "Calling biasedmf. Worker number: 8\n",
      "None\n",
      "Calling implicitmf. Worker number: 1\n",
      "Calling implicitmf. Worker number: 2\n",
      "Calling implicitmf. Worker number: 3\n",
      "Calling implicitmf. Worker number: 4\n",
      "Calling implicitmf. Worker number: 5\n",
      "Calling implicitmf. Worker number: 6\n",
      "Calling implicitmf. Worker number: 7\n",
      "Calling implicitmf. Worker number: 8\n",
      "None\n",
      "Calling funksvd. Worker number: 1\n",
      "Calling funksvd. Worker number: 2\n",
      "Calling funksvd. Worker number: 3\n",
      "Calling funksvd. Worker number: 4\n",
      "Calling funksvd. Worker number: 5\n",
      "Calling funksvd. Worker number: 6\n",
      "Calling funksvd. Worker number: 7\n",
      "Calling funksvd. Worker number: 8\n"
     ]
    }
   ],
   "source": [
    "warm_up(None, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gunicorn methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def get_gunicorn_master_pid():\n",
    "    proc1 = subprocess.Popen(['ps', 'ax'], stdout=subprocess.PIPE)\n",
    "    proc2 = subprocess.Popen(['grep', 'gunicorn'], stdin=proc1.stdout,\n",
    "                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    proc1.stdout.close() # Allow proc1 to receive a SIGPIPE if proc2 exits.\n",
    "    out, err = proc2.communicate()\n",
    "    master_id = out[:6].decode('utf-8').replace(' ', '')\n",
    "    return master_id\n",
    "\n",
    "def add_workers(n):\n",
    "    master_id = get_gunicorn_master_pid()\n",
    "    for i in range(n):\n",
    "        os.system(f\"kill -s TTIN {master_id}\")\n",
    "        \n",
    "def remove_workers(n):\n",
    "    master_id = get_gunicorn_master_pid()\n",
    "    for i in range(n):\n",
    "        os.system(f\"kill -s TTOU {master_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call predict and recommend from server for canonical config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions for different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: bias\n",
      "Number of requests: 10\n",
      "Total response time: 0.45714673700058484\n",
      "Throughput (requests per second): 21.87481434432115\n",
      "Peak response time: 0.31431903399970906\n",
      "Mean response time: 0.2425812477999898\n",
      "99 percentile: 0.31350228039975264\n",
      "---------------------\n",
      "\n",
      "Algorithm: itemitem\n",
      "Number of requests: 10\n",
      "Total response time: 5.706992383000397\n",
      "Throughput (requests per second): 1.7522364371446024\n",
      "Peak response time: 3.9596992719998525\n",
      "Mean response time: 3.080569376800031\n",
      "99 percentile: 3.9589826475398606\n",
      "---------------------\n",
      "\n",
      "Algorithm: useruser\n",
      "Number of requests: 10\n",
      "Total response time: 0.5534656159998121\n",
      "Throughput (requests per second): 18.067969736359185\n",
      "Peak response time: 0.39621907499986264\n",
      "Mean response time: 0.2914110841001275\n",
      "99 percentile: 0.39601378967980966\n",
      "---------------------\n",
      "\n",
      "Algorithm: biasedmf\n",
      "Number of requests: 10\n",
      "Total response time: 0.6948380330004511\n",
      "Throughput (requests per second): 14.39184317072855\n",
      "Peak response time: 0.49662871600048675\n",
      "Mean response time: 0.3662705369998548\n",
      "99 percentile: 0.49641401272041547\n",
      "---------------------\n",
      "\n",
      "Algorithm: implicitmf\n",
      "Number of requests: 10\n",
      "Total response time: 0.5208503329995438\n",
      "Throughput (requests per second): 19.199373344758445\n",
      "Peak response time: 0.3850054499998805\n",
      "Mean response time: 0.29285978199977764\n",
      "99 percentile: 0.38397533969985487\n",
      "---------------------\n",
      "\n",
      "Algorithm: funksvd\n",
      "Number of requests: 10\n",
      "Total response time: 0.5066038659997503\n",
      "Throughput (requests per second): 19.739288764142454\n",
      "Peak response time: 0.3378942620001908\n",
      "Mean response time: 0.2671147672001098\n",
      "99 percentile: 0.3377962539802229\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for algo in pred_algos:\n",
    "    file_name = f'preds_{algo}_parallel_threads_8_workers_4_num_req_{num_requests}.csv'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    print(f'Algorithm: {algo}')\n",
    "    future = asyncio.ensure_future(get_preds_sem(8, algo, file_name, True))\n",
    "    loop.run_until_complete(future)\n",
    "#    plot_numbers(file_name)\n",
    "#    hist_numbers(file_name)\n",
    "    print('---------------------')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of requests: 10\n",
      "Total response time: 0.06893939900055557\n",
      "Throughput (requests per second): 145.0549344057875\n",
      "Peak response time: 0.052158723000502505\n",
      "Mean response time: 0.03669173280022733\n",
      "99 percentile: 0.05120062710047023\n"
     ]
    }
   ],
   "source": [
    "algo_rec = 'popular'\n",
    "file_name = f'recs_{algo_rec}_parallel_threads_8_workers_4_num_req_{num_requests}.csv'\n",
    "loop = asyncio.get_event_loop()\n",
    "future = asyncio.ensure_future(get_recs_sem(8, algo_rec, file_name))\n",
    "loop.run_until_complete(future)\n",
    "#plot_numbers(file_name)\n",
    "#hist_numbers(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speedup Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughputs = []\n",
    "linear_speedup_algos = ['biasedmf', 'itemitem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_server(file_name):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_preds_sem(8, current_algo, file_name, True))\n",
    "    loop.run_until_complete(future)\n",
    "#    plot_numbers(file_name)\n",
    "#    hist_numbers(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_config = [1, 2, 4] #, 8, 12, 16, 24]\n",
    "inc_config = [1, 2, 4] #, 4, 4, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo: biasedmf, Workers: 1\n",
      "Number of requests: 10\n",
      "Total response time: 1.750941180999689\n",
      "Throughput (requests per second): 5.711214122161752\n",
      "Peak response time: 1.4139431200001127\n",
      "Mean response time: 0.9230218484998659\n",
      "99 percentile: 1.4135731476400724\n",
      "add 1 workers\n",
      "------------------\n",
      "Algo: biasedmf, Workers: 2\n",
      "Number of requests: 10\n",
      "Total response time: 1.6936998350001886\n",
      "Throughput (requests per second): 5.904233910490336\n",
      "Peak response time: 1.3419794239998737\n",
      "Mean response time: 0.8791842122998788\n",
      "99 percentile: 1.341489655969817\n",
      "add 2 workers\n",
      "------------------\n",
      "Algo: biasedmf, Workers: 4\n",
      "Number of requests: 10\n",
      "Total response time: 0.8475143490004484\n",
      "Throughput (requests per second): 11.799210257376668\n",
      "Peak response time: 0.6402733830000216\n",
      "Mean response time: 0.4171475012998599\n",
      "99 percentile: 0.6389676171899555\n",
      "------------------\n",
      "*************************************************\n",
      "Algo: itemitem, Workers: 1\n",
      "Number of requests: 10\n",
      "Total response time: 15.830987913999707\n",
      "Throughput (requests per second): 0.6316725181223068\n",
      "Peak response time: 12.764002353999786\n",
      "Mean response time: 8.11847501209986\n",
      "99 percentile: 12.74998959906975\n",
      "add 1 workers\n",
      "------------------\n",
      "Algo: itemitem, Workers: 2\n",
      "Number of requests: 10\n",
      "Total response time: 8.099739059000058\n",
      "Throughput (requests per second): 1.2346076740445682\n",
      "Peak response time: 6.458625287999894\n",
      "Mean response time: 4.495799038600216\n",
      "99 percentile: 6.455574980729943\n",
      "add 2 workers\n",
      "------------------\n",
      "Algo: itemitem, Workers: 4\n",
      "Number of requests: 10\n",
      "Total response time: 5.699196462000145\n",
      "Throughput (requests per second): 1.7546333183415963\n",
      "Peak response time: 3.9835724259992276\n",
      "Mean response time: 3.1213374221000776\n",
      "99 percentile: 3.982095828669326\n",
      "------------------\n",
      "*************************************************\n"
     ]
    }
   ],
   "source": [
    "for current_algo in linear_speedup_algos:\n",
    "    i = 0\n",
    "    throughputs = []\n",
    "    remove_workers(3) # reduce from 4 workers to 1\n",
    "    for num_workers in workers_config:\n",
    "        print(f'Algo: {current_algo}, Workers: {num_workers}')\n",
    "        warm_up(current_algo, num_workers, display_logs=False)\n",
    "        file_name = f'preds_{current_algo}_parallel_threads_8_workers_{num_workers}_num_req_{num_requests}.csv'\n",
    "        call_server(file_name)\n",
    "        if (num_workers != workers_config[-1]):\n",
    "            print(f'add {inc_config[i]} workers')\n",
    "            add_workers(inc_config[i])\n",
    "        i += 1\n",
    "        print('------------------')\n",
    "    throughput_file_name_workers = f'throughput_single_multiple_workers_algo_{current_algo}.csv'\n",
    "    np.savetxt(throughput_file_name_workers, throughputs , delimiter=',')\n",
    "    remove_workers(workers_config[-1] - 4) # remove workers to get only 4 (default config)\n",
    "    print('*******************************************************')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Throughput by number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throughput_file_name_workers = 'throughput_single_multiple_workers.csv'\n",
    "# np.savetxt(throughput_file_name_workers, throughputs , delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throughputs_workers_from_file = np.genfromtxt(throughput_file_name_workers, delimiter=',')\n",
    "# workers = [1, 2, 4, 8, 16]\n",
    "# y_pos = np.arange(len(throughputs_workers_from_file))\n",
    "\n",
    "# plt.bar(y_pos, throughputs_workers_from_file, align='center', alpha=0.5)\n",
    "# plt.xticks(y_pos, workers)\n",
    "# plt.ylabel('Throughput')\n",
    "# plt.title('Throughput by workers')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lenskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "from binpickle import BinPickleFile\n",
    "from pathlib import Path\n",
    "\n",
    "directory_path = 'models'\n",
    "algo_pred_lkpy = 'bias.bpk'\n",
    "\n",
    "def load_for_shared_mem(file_name):\n",
    "    full_file_name = Path(directory_path) / file_name\n",
    "\n",
    "    binpickle_file = BinPickleFile(full_file_name, direct=True)\n",
    "    model = binpickle_file.load()\n",
    "    return model\n",
    "\n",
    "def get_predictions_from_model(model, user, items):\n",
    "    try:\n",
    "        results = []\n",
    "        df_preds = model.predict_for_user(user, items)\n",
    "        for index, value in df_preds.iteritems():\n",
    "            if not math.isnan(value):\n",
    "                results.append({'item': index, 'score': value})\n",
    "        return results\n",
    "    except:\n",
    "        print(f\"Unexpected preds error for user: {user}, with items: {items}. Error: {sys.exc_info()[0]}\")\n",
    "        raise\n",
    "        \n",
    "\n",
    "# Predictions    \n",
    "async def get_preds_threads_lkpy(num_sem, model, file_name=None, add_throughput=False):\n",
    "    times = []\n",
    "    sem = asyncio.Semaphore(num_sem)\n",
    "    tasks = []    \n",
    "    num_requests = len(n_rand_users)\n",
    "    print(f'Number of requests: {num_requests}')\n",
    "    start_preds = perf_counter()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in n_rand_users.iterrows():\n",
    "            task = asyncio.ensure_future(get_user_preds_with_threads_lkpy(row['user'], algo_pred_lkpy, items, session, sem, times, model))\n",
    "            tasks.append(task)         \n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        time_taken_all = perf_counter() - start_preds\n",
    "        print_stats(times, time_taken_all, num_requests)\n",
    "        \n",
    "        if file_name != None and file_name != '':\n",
    "            if os.path.exists(file_name):\n",
    "                os.remove(file_name)\n",
    "            np.asarray(times)\n",
    "            np.savetxt(file_name, times, delimiter=',')\n",
    "        \n",
    "        if add_throughput:\n",
    "            throughputs.append(num_requests / time_taken_all)\n",
    "\n",
    "async def get_user_preds_with_threads_lkpy(user, algo, items, session, sem, times, model):\n",
    "    async with sem:  # semaphore limits num of simultaneous downloads\n",
    "        return await get_user_preds_threads_lkpy(user, algo, items, session, times, model)        \n",
    "        \n",
    "async def get_user_preds_threads_lkpy(user, algo, items, session, times, model):\n",
    "    try:\n",
    "        start = perf_counter()\n",
    "        results = []\n",
    "        df_preds = model.predict_for_user(user, items.split(','))\n",
    "        for index, value in df_preds.iteritems():\n",
    "            if not math.isnan(value):\n",
    "                results.append({'item': index, 'score': value})\n",
    "                \n",
    "        time_taken = perf_counter() - start\n",
    "        times.append(time_taken)\n",
    "        return results\n",
    "    except:\n",
    "        print(f\"Unexpected preds error for user: {user}, with items: {items}. Error: {sys.exc_info()[0]}\")\n",
    "        raise        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_save_model\n",
    "algos = \"bias, biasedmf\" #, itemitem\"\n",
    "train_save_model.save_models(algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lk_recserver_algos = ['bias', 'biasedmf', 'itemitem']\n",
    "for lk_recserver_algo in lk_recserver_algos:\n",
    "    print(f'Algo: {lk_recserver_algo}')\n",
    "    print('Lenskit performance:')\n",
    "    model = load_for_shared_mem(f'{lk_recserver_algo}.bpk')\n",
    "    file_name = f'lkpy_parallel_threads_8_{lk_recserver_algo}__num_req_{num_requests}.csv'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_preds_threads_lkpy(8, model, file_name))\n",
    "    loop.run_until_complete(future)\n",
    "    #plot_numbers(file_name)\n",
    "    #hist_numbers(file_name)\n",
    "    print('------------------')    \n",
    "    warm_up(algo, 8, False)\n",
    "    print('Recommendation server performance:')\n",
    "    file_name = f'preds_{lk_recserver_algo}_parallel_threads_8_workers_4_num_req_{num_requests}.csv'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_preds_sem(8, algo, file_name, True))\n",
    "    loop.run_until_complete(future)\n",
    "    #plot_numbers(file_name)\n",
    "    #hist_numbers(file_name)\n",
    "    print('*******************************************************')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
